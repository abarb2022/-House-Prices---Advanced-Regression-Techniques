{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:36.372404Z","iopub.execute_input":"2025-04-11T09:19:36.372679Z","iopub.status.idle":"2025-04-11T09:19:36.830609Z","shell.execute_reply.started":"2025-04-11T09:19:36.372647Z","shell.execute_reply":"2025-04-11T09:19:36.829346Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)  \npd.set_option('display.width', None)        \npd.set_option('display.expand_frame_repr', False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:36.831657Z","iopub.execute_input":"2025-04-11T09:19:36.832165Z","iopub.status.idle":"2025-04-11T09:19:36.839354Z","shell.execute_reply.started":"2025-04-11T09:19:36.832129Z","shell.execute_reply":"2025-04-11T09:19:36.838129Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:36.840254Z","iopub.execute_input":"2025-04-11T09:19:36.840529Z","iopub.status.idle":"2025-04-11T09:19:36.920138Z","shell.execute_reply.started":"2025-04-11T09:19:36.840505Z","shell.execute_reply":"2025-04-11T09:19:36.919232Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:36.921229Z","iopub.execute_input":"2025-04-11T09:19:36.921555Z","iopub.status.idle":"2025-04-11T09:19:37.480817Z","shell.execute_reply.started":"2025-04-11T09:19:36.921526Z","shell.execute_reply":"2025-04-11T09:19:37.479896Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_ids = x_train.pop('Id')\ntest_ids = x_test.pop('Id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:37.481803Z","iopub.execute_input":"2025-04-11T09:19:37.482178Z","iopub.status.idle":"2025-04-11T09:19:37.488304Z","shell.execute_reply.started":"2025-04-11T09:19:37.482157Z","shell.execute_reply":"2025-04-11T09:19:37.487349Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"x_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:37.491267Z","iopub.execute_input":"2025-04-11T09:19:37.491505Z","iopub.status.idle":"2025-04-11T09:19:37.681422Z","shell.execute_reply.started":"2025-04-11T09:19:37.491485Z","shell.execute_reply":"2025-04-11T09:19:37.680317Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 1168 entries, 254 to 1126\nData columns (total 79 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   MSSubClass     1168 non-null   int64  \n 1   MSZoning       1168 non-null   object \n 2   LotFrontage    951 non-null    float64\n 3   LotArea        1168 non-null   int64  \n 4   Street         1168 non-null   object \n 5   Alley          74 non-null     object \n 6   LotShape       1168 non-null   object \n 7   LandContour    1168 non-null   object \n 8   Utilities      1168 non-null   object \n 9   LotConfig      1168 non-null   object \n 10  LandSlope      1168 non-null   object \n 11  Neighborhood   1168 non-null   object \n 12  Condition1     1168 non-null   object \n 13  Condition2     1168 non-null   object \n 14  BldgType       1168 non-null   object \n 15  HouseStyle     1168 non-null   object \n 16  OverallQual    1168 non-null   int64  \n 17  OverallCond    1168 non-null   int64  \n 18  YearBuilt      1168 non-null   int64  \n 19  YearRemodAdd   1168 non-null   int64  \n 20  RoofStyle      1168 non-null   object \n 21  RoofMatl       1168 non-null   object \n 22  Exterior1st    1168 non-null   object \n 23  Exterior2nd    1168 non-null   object \n 24  MasVnrType     485 non-null    object \n 25  MasVnrArea     1162 non-null   float64\n 26  ExterQual      1168 non-null   object \n 27  ExterCond      1168 non-null   object \n 28  Foundation     1168 non-null   object \n 29  BsmtQual       1140 non-null   object \n 30  BsmtCond       1140 non-null   object \n 31  BsmtExposure   1140 non-null   object \n 32  BsmtFinType1   1140 non-null   object \n 33  BsmtFinSF1     1168 non-null   int64  \n 34  BsmtFinType2   1140 non-null   object \n 35  BsmtFinSF2     1168 non-null   int64  \n 36  BsmtUnfSF      1168 non-null   int64  \n 37  TotalBsmtSF    1168 non-null   int64  \n 38  Heating        1168 non-null   object \n 39  HeatingQC      1168 non-null   object \n 40  CentralAir     1168 non-null   object \n 41  Electrical     1167 non-null   object \n 42  1stFlrSF       1168 non-null   int64  \n 43  2ndFlrSF       1168 non-null   int64  \n 44  LowQualFinSF   1168 non-null   int64  \n 45  GrLivArea      1168 non-null   int64  \n 46  BsmtFullBath   1168 non-null   int64  \n 47  BsmtHalfBath   1168 non-null   int64  \n 48  FullBath       1168 non-null   int64  \n 49  HalfBath       1168 non-null   int64  \n 50  BedroomAbvGr   1168 non-null   int64  \n 51  KitchenAbvGr   1168 non-null   int64  \n 52  KitchenQual    1168 non-null   object \n 53  TotRmsAbvGrd   1168 non-null   int64  \n 54  Functional     1168 non-null   object \n 55  Fireplaces     1168 non-null   int64  \n 56  FireplaceQu    621 non-null    object \n 57  GarageType     1104 non-null   object \n 58  GarageYrBlt    1104 non-null   float64\n 59  GarageFinish   1104 non-null   object \n 60  GarageCars     1168 non-null   int64  \n 61  GarageArea     1168 non-null   int64  \n 62  GarageQual     1104 non-null   object \n 63  GarageCond     1104 non-null   object \n 64  PavedDrive     1168 non-null   object \n 65  WoodDeckSF     1168 non-null   int64  \n 66  OpenPorchSF    1168 non-null   int64  \n 67  EnclosedPorch  1168 non-null   int64  \n 68  3SsnPorch      1168 non-null   int64  \n 69  ScreenPorch    1168 non-null   int64  \n 70  PoolArea       1168 non-null   int64  \n 71  PoolQC         6 non-null      object \n 72  Fence          233 non-null    object \n 73  MiscFeature    46 non-null     object \n 74  MiscVal        1168 non-null   int64  \n 75  MoSold         1168 non-null   int64  \n 76  YrSold         1168 non-null   int64  \n 77  SaleType       1168 non-null   object \n 78  SaleCondition  1168 non-null   object \ndtypes: float64(3), int64(33), object(43)\nmemory usage: 730.0+ KB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"cat_cols = [col for col in x_train.columns if x_train[col].dtype == 'object']\nnum_cols = [col for col in x_train.columns if x_train[col].dtype != 'object']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:37.682306Z","iopub.execute_input":"2025-04-11T09:19:37.682551Z","iopub.status.idle":"2025-04-11T09:19:37.688041Z","shell.execute_reply.started":"2025-04-11T09:19:37.682531Z","shell.execute_reply":"2025-04-11T09:19:37.687015Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"threshold = 3\ns = x_train[cat_cols].nunique()\nordinal_columns = list(s[s > 3].index)\none_hot_columns = list(s[s <= 3].index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:37.689222Z","iopub.execute_input":"2025-04-11T09:19:37.689529Z","iopub.status.idle":"2025-04-11T09:19:37.712027Z","shell.execute_reply.started":"2025-04-11T09:19:37.689499Z","shell.execute_reply":"2025-04-11T09:19:37.710923Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**CLEANING**","metadata":{}},{"cell_type":"code","source":"fill_strategies = {\n    'Alley': 'none',\n    'BsmtQual': 'none',\n    'BsmtCond': 'none',\n    'BsmtExposure': 'none',\n    'BsmtFinType1': 'none',\n    'BsmtFinType2': 'none',\n    'FireplaceQu': 'none',\n    'GarageType': 'none',\n    'GarageFinish': 'none',\n    'GarageQual': 'none',\n    'GarageCond': 'none',\n    'PoolQC': 'none',\n    'Fence': 'none',\n    'MiscFeature': 'none',\n    'MasVnrType': 'none'\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:37.726756Z","iopub.execute_input":"2025-04-11T09:19:37.727122Z","iopub.status.idle":"2025-04-11T09:19:37.745391Z","shell.execute_reply.started":"2025-04-11T09:19:37.727094Z","shell.execute_reply":"2025-04-11T09:19:37.744410Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom typing import Optional, Dict, Union\nclass NullCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self, \n                 fill_strategies: Optional[Dict[str, Union[str, float, int]]] = None,\n                 default_numerical_strategy: str = 'mean',  \n                 default_categorical_strategy: str = 'mode',\n                 drop_threshold: float = 0.8):\n        \n        self.fill_strategies = fill_strategies or {}\n        self.default_numerical_strategy = default_numerical_strategy\n        self.default_categorical_strategy = default_categorical_strategy\n        self.drop_threshold = drop_threshold\n        self._fill_values = {}\n        self._dropped_columns = []\n    \n    def fit(self, X: pd.DataFrame, y=None):\n        total_rows = len(X)\n        self._dropped_columns = [\n            col for col in X.columns \n            if X[col].isna().sum() / total_rows >= self.drop_threshold\n        ]\n        \n        remaining_cols = [col for col in X.columns if col not in self._dropped_columns]\n        df_remaining = X[remaining_cols]\n        \n        self._fill_values = {}\n        \n        for col in df_remaining.columns:\n            if col in self.fill_strategies:\n                strategy = self.fill_strategies[col]\n            else:\n                if pd.api.types.is_numeric_dtype(df_remaining[col]):\n                    strategy = self.default_numerical_strategy\n                else:\n                    strategy = self.default_categorical_strategy\n            \n            if strategy == 'mode':\n                fill_value = df_remaining[col].mode()[0] if not df_remaining[col].mode().empty else None\n            elif strategy == 'median':\n                fill_value = df_remaining[col].median()\n            elif strategy == 'mean':\n                fill_value = df_remaining[col].mean()\n            elif strategy == 'none':\n                fill_value = 0 \n            else:\n                fill_value = strategy\n            \n            self._fill_values[col] = fill_value\n        \n        return self\n    \n        \n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        df_filled = X.drop(columns=self._dropped_columns, errors='ignore')\n\n        for col, fill_value in self._fill_values.items():\n            if col in df_filled.columns and fill_value is not None:\n                df_filled[col] = df_filled[col].fillna(fill_value)\n        \n        \n        return df_filled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:37.712939Z","iopub.execute_input":"2025-04-11T09:19:37.713181Z","iopub.status.idle":"2025-04-11T09:19:37.725600Z","shell.execute_reply.started":"2025-04-11T09:19:37.713163Z","shell.execute_reply":"2025-04-11T09:19:37.724471Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**FEATURE ENGINEERING**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass CustomPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, ordinal_columns, one_hot_columns):\n        self.ordinal_columns = ordinal_columns\n        self.one_hot_columns = one_hot_columns\n        self.ordinal_mappings_ = {}\n        self.onehot_columns_ = None\n        \n    def fit(self, X, y=None):\n        for col in self.ordinal_columns:\n            if col in X.columns:\n                unique_vals = X[col].dropna().unique()\n                self.ordinal_mappings_[col] = {val: idx for idx, val in enumerate(unique_vals)}\n        \n        if self.one_hot_columns:\n            temp = pd.get_dummies(X[self.one_hot_columns], drop_first=True)\n            self.onehot_columns_ = temp.columns.tolist()\n            \n        return self\n        \n    def transform(self, X):\n        X_transformed = X.copy()\n        \n        for col, mapping in self.ordinal_mappings_.items():\n            if col in X_transformed.columns:\n                X_transformed[col] = X_transformed[col].map(mapping).fillna(-1)\n        \n        if self.one_hot_columns and self.onehot_columns_:\n            dummies = pd.get_dummies(X_transformed[self.one_hot_columns], drop_first=True)\n            \n            for col in self.onehot_columns_:\n                if col not in dummies.columns:\n                    dummies[col] = 0\n            \n            X_transformed = pd.concat([\n                X_transformed.drop(self.one_hot_columns, axis=1),\n                dummies[self.onehot_columns_]  # Maintain consistent column order\n            ], axis=1)\n            \n        for col in X_transformed.columns:\n            if X_transformed[col].dtype == object:\n                X_transformed[col] = pd.to_numeric(X_transformed[col], errors='coerce').fillna(0)\n                \n        return X_transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:37.746456Z","iopub.execute_input":"2025-04-11T09:19:37.746883Z","iopub.status.idle":"2025-04-11T09:19:37.807124Z","shell.execute_reply.started":"2025-04-11T09:19:37.746851Z","shell.execute_reply":"2025-04-11T09:19:37.806085Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**FEATURE SELECTION AND TRAINING**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.compose import ColumnTransformer\n\ngb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n\nrf = RandomForestRegressor(\n    n_estimators=200,\n    max_depth=7,\n    min_samples_split=5,\n    random_state=42,\n    n_jobs=-1\n)\n\n\nxgb = XGBRegressor(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=5,\n    reg_lambda=1.0,  \n    reg_alpha=0.5,   \n    subsample=0.8,\n    random_state=42\n)\nparam_grid = {\n    'scaler': [StandardScaler(), MinMaxScaler()]\n}\n\ncleaner = NullCleaner(\n    fill_strategies=fill_strategies, \n    default_numerical_strategy='mean',\n    default_categorical_strategy='mode',\n    drop_threshold=0.8\n)\nx_cleaned = cleaner.fit_transform(x_train)\n\n\nordinal_columns = [col for col in ordinal_columns if col in x_cleaned]\none_hot_columns = [col for col in one_hot_columns if col in x_cleaned]\n\npipeline = Pipeline([\n    ('cleaner', cleaner),\n    ('preprocessor', CustomPreprocessor(\n        ordinal_columns=ordinal_columns,\n        one_hot_columns=one_hot_columns\n    )),\n    ('scaler', StandardScaler()),\n    #('feature_select', RFE(estimator=gb, n_features_to_select=60, step=1)),\n    #('feature_select', RFE(estimator=LinearRegression(), n_features_to_select=60, step=1)),\n    ('feature_select', RFE(estimator=LinearRegression(), n_features_to_select=60, step=1)),\n    ('model', LinearRegression()),\n    #('feature_select', RFE(estimator=rf, n_features_to_select=60, step=1)),\n    #('model', rf)\n])\n\n\n\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(\n    pipeline,\n    param_grid=param_grid,\n    cv=kfold,\n    scoring='neg_mean_squared_error',  \n    verbose=2,\n    return_train_score=True\n)\n\n\ngrid_search.fit(x_train, np.log1p(y_train))\nbest_pipeline = grid_search.best_estimator_\ncv_results = grid_search.cv_results_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:37.808127Z","iopub.execute_input":"2025-04-11T09:19:37.808471Z","iopub.status.idle":"2025-04-11T09:19:42.513822Z","shell.execute_reply.started":"2025-04-11T09:19:37.808443Z","shell.execute_reply":"2025-04-11T09:19:42.513134Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 2 candidates, totalling 10 fits\n[CV] END ............................scaler=StandardScaler(); total time=   0.8s\n[CV] END ............................scaler=StandardScaler(); total time=   0.3s\n[CV] END ............................scaler=StandardScaler(); total time=   0.3s\n[CV] END ............................scaler=StandardScaler(); total time=   0.3s\n[CV] END ............................scaler=StandardScaler(); total time=   0.3s\n[CV] END ..............................scaler=MinMaxScaler(); total time=   0.3s\n[CV] END ..............................scaler=MinMaxScaler(); total time=   0.3s\n[CV] END ..............................scaler=MinMaxScaler(); total time=   0.3s\n[CV] END ..............................scaler=MinMaxScaler(); total time=   0.3s\n[CV] END ..............................scaler=MinMaxScaler(); total time=   0.3s\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def log_rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n\ntrain_scores = -cv_results['mean_train_score']  \nval_scores = -cv_results['mean_test_score']\n\ntrain_rmse = np.sqrt(train_scores).mean()  \nval_rmse = np.sqrt(val_scores).mean()     \n\ny_test_pred = np.expm1(best_pipeline.predict(x_test))\ntest_rmse = log_rmse(y_test, y_test_pred)\n\nprint(\"\\n=== Overfitting/Underfitting Analysis ===\")\nprint(f\"Training RMSE (avg CV folds): {train_rmse:.6f}\")\nprint(f\"Validation RMSE (avg CV folds): {val_rmse:.6f}\")\nprint(f\"Test RMSE: {test_rmse:.6f}\")\n\nif train_rmse < 0.7 * val_rmse:\n    print(\"\\n Training error much lower than validation error\")\nelif val_rmse > 1.3 * test_rmse:\n    print(\"\\n Validation error higher than test error\")\nelif train_rmse > 0.9 * val_rmse and val_rmse > 0.8:\n    print(\"\\n Both training and validation errors are high\")\nelse:\n    print(\"\\n Balanced Model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:19:42.514468Z","iopub.execute_input":"2025-04-11T09:19:42.514727Z","iopub.status.idle":"2025-04-11T09:19:42.601535Z","shell.execute_reply.started":"2025-04-11T09:19:42.514706Z","shell.execute_reply":"2025-04-11T09:19:42.600150Z"}},"outputs":[{"name":"stdout","text":"\n=== Overfitting/Underfitting Analysis ===\nTraining RMSE (avg CV folds): 0.129568\nValidation RMSE (avg CV folds): 0.156797\nTest RMSE: 0.148503\n\n Balanced Model\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install mlflow dagshub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:13:10.149872Z","iopub.status.idle":"2025-04-11T09:13:10.150141Z","shell.execute_reply.started":"2025-04-11T09:13:10.150015Z","shell.execute_reply":"2025-04-11T09:13:10.150026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='abarb22', repo_name='-House-Prices---Advanced-Regression-Techniques', mlflow=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:13:10.150977Z","iopub.status.idle":"2025-04-11T09:13:10.151225Z","shell.execute_reply.started":"2025-04-11T09:13:10.151107Z","shell.execute_reply":"2025-04-11T09:13:10.151118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mlflow\nimport mlflow.sklearn\nimport platform\nfrom sklearn.model_selection import learning_curve\nimport matplotlib.pyplot as plt\n\n\nwith mlflow.start_run():\n    mlflow.log_param(\"model_type\", \"LinearRegression\")\n    mlflow.log_param(\"feature_selection\", \"RFE\")\n    mlflow.log_param(\"n_features_selected\", 60)  \n    mlflow.log_param(\"preprocessing\", \"OrdinalEncoding+OneHotEncoding\")\n    mlflow.log_param(\"scaler_options\", [scaler.__class__.__name__ for scaler in param_grid['scaler']])\n\n    grid_search.fit(x_train, np.log1p(y_train))\n    best_pipeline = grid_search.best_estimator_\n    \n    cv_results = grid_search.cv_results_\n    train_rmse = np.sqrt(-cv_results['mean_train_score']).mean()\n    val_rmse = np.sqrt(-cv_results['mean_test_score']).mean()\n    \n    y_test_pred = np.expm1(best_pipeline.predict(x_test))\n    test_rmse = log_rmse(y_test, y_test_pred)\n    \n    gap_train_val = abs(train_rmse - val_rmse)\n    gap_val_test = abs(val_rmse - test_rmse)\n    \n    for param, value in grid_search.best_params_.items():\n        mlflow.log_param(f\"best_{param}\", value.__class__.__name__)\n    \n    mlflow.log_metrics({\n        \"train_log_rmse\": train_rmse,\n        \"validation_log_rmse\": val_rmse,\n        \"test_log_rmse\": test_rmse,\n        \"gap_train_validation\": gap_train_val,\n        \"gap_validation_test\": gap_val_test\n    })\n    \n    mlflow.sklearn.log_model(best_pipeline, \"pipeline_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:13:10.151967Z","iopub.status.idle":"2025-04-11T09:13:10.152214Z","shell.execute_reply.started":"2025-04-11T09:13:10.152100Z","shell.execute_reply":"2025-04-11T09:13:10.152111Z"}},"outputs":[],"execution_count":null}]}